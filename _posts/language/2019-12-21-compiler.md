---
layout: post
title:  "Compilerbau"
date:   2019-12-21 10:10:00
categories: language
---

## Lexikalische Analyse

Damit der Parser es leichter hat wird der Eingabetext zerlegt in einen Stream von Tokens. Diese haben zwei Attribute: Typ und ggf. Wert.

Zum erkennen der Tokens werden **Reguläre Ausdrücke** verwendet.

Zur Implementierung von Regulären Ausdrücken werden **Endliche Automaten** verwendet.

Man kann Übersetzen: RegExp -> NFA (nicht deterministisch, Epsilon) -> DFA (deterministisch)

Es gibt Algorithmen zur Minimierung von Automaten.

Ein bekanntes Tool für die Lexikalische Analyse ist **Lex**

Es hat das Prinzip **Maximal Munch** oder auch Longest Match, das besagt es soll so viel wie möglich vom Eingabestrom konsumiert werden.
