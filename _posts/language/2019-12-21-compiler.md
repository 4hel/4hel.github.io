---
layout: post
title:  "Compilerbau"
date:   2019-12-21 10:10:00
categories: language
---

## Lexikalische Analyse

Damit der Parser es leichter hat wird der Eingabetext zerlegt in einen Stream von Tokens. Diese haben zwei Attribute: Typ und ggf. Wert.

Zum erkennen der Tokens werden **Reguläre Ausdrücke** verwendet.

Zur Implementierung von Regulären Ausdrücken werden **Endliche Automaten** verwendet.

Man kann Übersetzen: RegExp -> NFA (nicht deterministisch, Epsilon) -> DFA (deterministisch)

Es gibt Algorithmen zur Minimierung von Automaten.

Ein bekanntes Tool für die Lexikalische Analyse ist **Lex**

Es hat das Prinzip **Maximal Munch** oder auch Longest Match, das besagt es soll so viel wie möglich vom Eingabestrom konsumiert werden.


## Syntaktische Analyse / Parsing

Parser liest Token-Stream und erzeugt Baumstruktur

Es gibt mehrdeutige Grammatiken, die für einen String verschiedene Parsebäume zulassen. So eine **ambiguous grammar** kann nicht verwendet werden. Siehe auch: [https://en.wikipedia.org/wiki/Dangling_else](https://en.wikipedia.org/wiki/Dangling_else)

**Grammar Flowchart** von Niklaus Wirth

![grammar](/img/language/compiler/grammar.gif)
